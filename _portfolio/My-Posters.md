---
title: "My take on various techniques"
excerpt: "LLMs, Activations, ... Deep Learning."
collection: portfolio
---

diffusion 등 이런 기술들에대한 페이퍼는... 무시해도 될 거 같긴해. 광전효과 이런 거도 다 대응되는 페이퍼가 있는데 안 보잖아. 매우 specific한 것들만. 

### 1. Paper

- Mathematics
	- Using $\pi$ digits to Generate Random Numbers: A Visual and Statistical Analysis
    - Sophie Germain's FLT


	- An Efficient and Generalizable Symbolic Regression Method for Time Series Analysis
	- Observation of Second Decay Chain from Nihonium
	- BHT algorithm
	- Monte Carlo Cryptography in logistic.
	- Trajectoid
	- Thermodynamic efficiency of Hamstring
	- Feynman's Keplerian Motion

- Rating
    - TrueSkill™ : A Bayesian Skill Rating System

8. System II
AutoCoT
Visual CoT
LLM cannot self-correct reasoning yet
OpenAI o1

- ControlNet
- MoBA(Mixture of Block Attention for Long-Context LLMs)
- LLaDA
- Training LLMs to Reason in a Continuous Latent Space
- Self-Consistency Improves CoT Reasoning in LMs
- CoT reasoning without prompting
- Differential Transformer
- Self-Rewarding LM
- CoT Prompting Elicits Reasoning in LLMs
- RAG : RAG for Knowledge-Intensive NLP tasks
- RoPE, RoFormer : Enhanced Transformer with RoPE
- LLaVA : Visual Instruction Tuning
- DPO : Your LM is Secretly a Reward Model
- GQA : Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
- RLHF, InstructGPT : Training LMs to Follow Instructions with Human Feedback

rlhf

alphafold

scaling laws
diffusion models
normalizing flows
VAE
hopfield network
information bottleneck theory
neural style transfer
imagenet
chinchilla optimality

FLAN
CLIP BLIP BLIPv2


- dLLM


### 4. Interesting Lectures and Videos

- ACM Talk, A new NN for optimal time series processing    
- JAX Talk, Generating Extremely Long Sequences with S4    
- Simons Institute, Simran Arora, Understanding and Improving Efficient LMs
- Yann LeCun, Lex Fridman Podcast
- Songlin Yang, Linear Attention and Beyond
- ASAP seminars

### 5. Blogs & Documents
- Bitter Lesson
- Situational Awareness